# Chatbot RAG com Azure OpenAI e FAISS para Consulta Inteligente de PDFs
O projeto nÃ£o possui um nome especÃ­fico, resolvi explicÃ¡-lo

## ğŸ“– DescriÃ§Ã£o do Projeto

Este projeto implementa um chatbot inteligente baseado na arquitetura RAG (Retrieval-Augmented Generation), capaz de responder perguntas com base no conteÃºdo de documentos PDF.

A soluÃ§Ã£o utiliza Azure OpenAI para geraÃ§Ã£o de embeddings e respostas contextuais, FAISS para indexaÃ§Ã£o vetorial de alta performance e FastAPI para exposiÃ§Ã£o de uma API REST profissional.

O sistema funciona em quatro etapas principais:

1. IngestÃ£o de documentos PDF
2. SegmentaÃ§Ã£o dos textos em chunks semÃ¢nticos
3. GeraÃ§Ã£o de embeddings utilizando Azure OpenAI
4. RecuperaÃ§Ã£o vetorial e geraÃ§Ã£o de respostas contextuais com modelo GPT

Ao receber uma pergunta, o sistema realiza uma busca semÃ¢ntica nos documentos indexados e utiliza o modelo de linguagem para gerar uma resposta precisa e fundamentada no conteÃºdo fornecido.

Este projeto demonstra competÃªncias prÃ¡ticas em:

- InteligÃªncia Artificial Generativa
- Arquitetura RAG
- Azure OpenAI Service
- Embeddings e busca vetorial
- Engenharia de prompts
- ConstruÃ§Ã£o de APIs com FastAPI
- Processamento de documentos PDF
- IntegraÃ§Ã£o com serviÃ§os em nuvem

Este projeto foi desenvolvido como parte do Microsoft Certification Challenge (DP-100), com foco na construÃ§Ã£o de soluÃ§Ãµes reais de Machine Learning e IA no Azure.

O resultado Ã© um sistema escalÃ¡vel, modular e pronto para aplicaÃ§Ãµes reais como:

- Assistentes acadÃªmicos
- Sistemas de suporte tÃ©cnico
- Chatbots corporativos
- Sistemas de consulta documental
- Ferramentas de auxÃ­lio para pesquisa cientÃ­fica

## ğŸ—ï¸ Arquitetura do Sistema

O sistema implementa um pipeline de Retrieval-Augmented Generation (RAG) utilizando Azure OpenAI para responder perguntas com base em documentos PDF.

Fluxo de funcionamento:

1. IngestÃ£o de documentos
   - PDFs sÃ£o carregados e convertidos em texto
   - O texto Ã© dividido em chunks menores

2. GeraÃ§Ã£o de embeddings
   - Cada chunk Ã© convertido em vetor usando Azure OpenAI Embeddings
   - Os vetores sÃ£o armazenados em um banco vetorial (ChromaDB)

3. Consulta do usuÃ¡rio
   - A pergunta do usuÃ¡rio Ã© convertida em embedding
   - O sistema busca os chunks mais relevantes no banco vetorial

4. GeraÃ§Ã£o da resposta
   - Os chunks relevantes sÃ£o enviados como contexto para o modelo GPT-4o
   - O modelo gera uma resposta baseada exclusivamente no conteÃºdo recuperado

Arquitetura simplificada:

User â†’ FastAPI â†’ Embedding â†’ Vector Store â†’ Context Retrieval â†’ GPT-4o â†’ Response

## ğŸ§° Stack TecnolÃ³gica

Este projeto foi construÃ­do utilizando tecnologias modernas para implementaÃ§Ã£o de sistemas de IA baseados em Retrieval-Augmented Generation (RAG).

### â˜ï¸ Cloud & IA

- Azure OpenAI Service  
  - Modelo de geraÃ§Ã£o: GPT-4o  
  - Modelo de embeddings: text-embedding-3-large  

### ğŸ§  Processamento e IA

- OpenAI Python SDK  
- ChromaDB (Vector Database)  
- Sentence chunking e embedding pipeline  

### âš™ï¸ Backend

- Python 3.10+
- FastAPI
- Uvicorn

### ğŸ“„ Processamento de documentos

- PyPDF
- Pipeline de ingestÃ£o e segmentaÃ§Ã£o de texto

### ğŸ”§ Ferramentas auxiliares

- python-dotenv (gerenciamento de variÃ¡veis de ambiente)
- Git e GitHub (versionamento)
- Ambiente virtual Python (venv)

### ğŸ§ª Ambiente de desenvolvimento

- Visual Studio Code
- Windows 11

## ğŸ“ Estrutura do Projeto

A estrutura do projeto segue boas prÃ¡ticas de organizaÃ§Ã£o, separando responsabilidades entre ingestÃ£o de dados, lÃ³gica de IA, API e armazenamento vetorial.

```
chatbot-pdf-rag-azure/
â”‚
â”œâ”€â”€ inputs/                     # PDFs utilizados como base de conhecimento
â”‚   â”œâ”€â”€ redes_fundamentos.pdf
â”‚   â””â”€â”€ redes_topologias_protocolos.pdf
â”‚
â”œâ”€â”€ outputs/                    # Vector store persistido
â”‚   â”œâ”€â”€ vector_index.faiss
â”‚   â””â”€â”€ metadata.pkl
â”‚
â”œâ”€â”€ src/                        # CÃ³digo-fonte principal
â”‚   â”œâ”€â”€ __init__.py             # Inicializa o mÃ³dulo Python
â”‚   â”œâ”€â”€ api.py                  # API REST com FastAPI
â”‚   â”œâ”€â”€ chatbot.py              # LÃ³gica principal do chatbot (RAG)
â”‚   â”œâ”€â”€ pdf_loader.py           # Carregamento de PDFs
â”‚   â”œâ”€â”€ text_chunker.py         # DivisÃ£o de texto em chunks
â”‚   â””â”€â”€ vector_store.py         # CriaÃ§Ã£o e persistÃªncia do Ã­ndice vetorial
â”‚
â”œâ”€â”€ notebooks/                  # Experimentos e testes (vazia)
â”‚
â”œâ”€â”€ docs/                       # DocumentaÃ§Ã£o tÃ©cnica (vazia pela simplicidade do projeto)
â”‚
â”œâ”€â”€ requirements.txt            # DependÃªncias do projeto
â”‚
â”œâ”€â”€ .env                        # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .gitignore                  # Arquivos ignorados pelo Git
â””â”€â”€ README.md                   # DocumentaÃ§Ã£o principal do projeto
```
### ğŸ“Œ DescriÃ§Ã£o dos componentes

- src/api.py  
  ResponsÃ¡vel por expor o chatbot atravÃ©s de uma API REST usando FastAPI.

- src/chatbot.py  
  Implementa o pipeline RAG completo:
  - Recebe perguntas
  - Busca documentos relevantes no vector store
  - Envia contexto para o modelo GPT-4o
  - Retorna resposta contextualizada

- src/create_vector_store.py  
  Script responsÃ¡vel por:
  - Ler arquivos PDF
  - Dividir em chunks
  - Gerar embeddings
  - Armazenar no ChromaDB

- chroma_db/  
  ContÃ©m o banco vetorial persistente, permitindo reutilizaÃ§Ã£o sem necessidade de reprocessar os documentos.

- data/  
  DiretÃ³rio contendo os documentos fonte utilizados como base de conhecimento.

- .env  
  Armazena credenciais sensÃ­veis e configuraÃ§Ãµes do Azure OpenAI.

## ğŸš€ Como executar localmente

### 1. Clonar o repositÃ³rio

git clone https://github.com/SEU-USUARIO/chatbot-pdf-rag-azure.git

cd chatbot-pdf-rag-azure


### 2. Criar e ativar ambiente virtual

Windows:

python -m venv venv
venv\Scripts\activate

Linux / Mac:

python3 -m venv venv
source venv/bin/activate


### 3. Instalar dependÃªncias

pip install -r requirements.txt


### 4. Configurar variÃ¡veis de ambiente

Criar arquivo `.env` na raiz do projeto:

AZURE_OPENAI_API_KEY=your_api_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini


### 5. Adicionar PDFs na pasta inputs/

inputs/
 â”œâ”€â”€ seu_documento.pdf


### 6. Gerar vector store

python src/vector_store.py

SaÃ­da esperada:

outputs/vector_index.faiss
outputs/metadata.pkl


### 7. Executar API

uvicorn src.api:app --reload


### 8. Acessar API

Abrir no navegador:

http://127.0.0.1:8000/docs


Interface interativa disponÃ­vel via Swagger UI.

## ğŸ“¡ Endpoint da API

### Base URL

http://127.0.0.1:8000


### Endpoint de consulta

POST /ask


### DescriÃ§Ã£o

Recebe uma pergunta do usuÃ¡rio, realiza busca semÃ¢ntica no vector store (FAISS) e retorna uma resposta gerada pelo Azure OpenAI com base no conteÃºdo dos PDFs.


### Request Body

{
  "question": "O que Ã© uma rede LAN?"
}


### Response

{
  "answer": "Uma rede LAN (Local Area Network) Ã© uma rede de computadores que cobre uma Ã¡rea geogrÃ¡fica limitada, como uma residÃªncia, escola ou escritÃ³rio."
}


### Exemplo usando curl

curl -X POST "http://127.0.0.1:8000/ask" \
-H "Content-Type: application/json" \
-d "{\"question\": \"O que Ã© uma rede LAN?\"}"


### Exemplo usando Python

import requests

url = "http://127.0.0.1:8000/ask"

data = {
    "question": "O que Ã© uma rede LAN?"
}

response = requests.post(url, json=data)

print(response.json())


### Interface interativa

DisponÃ­vel em:

http://127.0.0.1:8000/docs

Permite testar a API diretamente pelo navegador.

## ğŸ’¬ Exemplos de Uso

### Exemplo 1 â€” Pergunta bÃ¡sica

Pergunta:

"O que Ã© uma rede LAN?"

Resposta:

"Uma rede LAN (Local Area Network) conecta dispositivos em uma Ã¡rea limitada, como uma casa ou escritÃ³rio."


---

### Exemplo 2 â€” Pergunta conceitual

Pergunta:

"Qual a diferenÃ§a entre LAN e WAN?"

Resposta:

"LAN cobre uma Ã¡rea local, enquanto WAN cobre grandes Ã¡reas geogrÃ¡ficas, conectando mÃºltiplas redes LAN."


---

### Exemplo 3 â€” Pergunta tÃ©cnica

Pergunta:

"O que Ã© o protocolo TCP?"

Resposta:

"TCP (Transmission Control Protocol) Ã© um protocolo de comunicaÃ§Ã£o confiÃ¡vel que garante a entrega correta e ordenada dos dados entre dispositivos."


---

### Exemplo 4 â€” Uso via Python

import requests

response = requests.post(
    "http://127.0.0.1:8000/ask",
    json={"question": "O que Ã© um roteador?"}
)

print(response.json())


---

### Exemplo 5 â€” Uso via curl

curl -X POST "http://127.0.0.1:8000/ask" \
-H "Content-Type: application/json" \
-d "{\"question\": \"O que Ã© um switch de rede?\"}"


---

### Exemplo 6 â€” Interface Web

Abra no navegador:

http://127.0.0.1:8000/docs

Permite testar perguntas diretamente na interface Swagger.

## ğŸš€ PossÃ­veis melhorias

Este projeto foi desenvolvido com arquitetura modular e permite diversas extensÃµes para nÃ­vel de produÃ§Ã£o.

### ğŸ§  Melhorias de IA

- Suporte a mÃºltiplos PDFs dinÃ¢micos via upload
- Uso de modelos mais avanÃ§ados (ex: GPT-4o)
- Re-ranking semÃ¢ntico para melhorar precisÃ£o das respostas
- Suporte a mÃºltiplos idiomas
- MemÃ³ria de conversa (contexto persistente)


---

### âš¡ Melhorias de performance

- Uso de banco vetorial dedicado:
  - Azure AI Search
  - Pinecone
  - Weaviate
  - Qdrant

- Cache de embeddings
- Processamento assÃ­ncrono
- ParalelizaÃ§Ã£o da geraÃ§Ã£o de embeddings


---

### ğŸŒ Melhorias de backend

- AutenticaÃ§Ã£o com JWT
- Rate limiting
- Logs estruturados
- Monitoramento com Azure Monitor
- Tratamento avanÃ§ado de erros


---

### ğŸ–¥ï¸ Melhorias de frontend

- Interface web com React ou Next.js
- Interface estilo ChatGPT
- Upload de arquivos via interface
- HistÃ³rico de conversas


---

### â˜ï¸ Deploy e produÃ§Ã£o

- ContainerizaÃ§Ã£o com Docker
- Deploy no Azure App Service
- Deploy no Azure Container Apps
- CI/CD com GitHub Actions
- Deploy serverless


---

### ğŸ§ª Melhorias de engenharia

- Testes automatizados (pytest)
- Arquitetura orientada a serviÃ§os
- ConfiguraÃ§Ã£o via variÃ¡veis de ambiente
- SeparaÃ§Ã£o entre ambiente dev e produÃ§Ã£o


---

Este projeto foi idealizado com base em boas prÃ¡ticas modernas de engenharia de IA e pode ser facilmente evoluÃ­do para um sistema de produÃ§Ã£o completo.

<br>
<br>

# RAG Chatbot with Azure OpenAI and FAISS for Intelligent PDF Querying
The project doesn't have a specific name, so I decided to explain it.

## ğŸ“– Project Description

This project implements an intelligent chatbot based on the Retrieval-Augmented Generation (RAG) architecture, capable of answering questions using the content of PDF documents.

The solution leverages Azure OpenAI for embeddings and contextual response generation, FAISS for high-performance vector indexing, and FastAPI to expose a professional REST API.

The system operates in four main stages:

1. PDF document ingestion
2. Text chunking into semantic segments
3. Embedding generation using Azure OpenAI
4. Vector retrieval and contextual response generation using GPT models

When a question is submitted, the system performs a semantic search across indexed documents and uses a language model to generate an accurate answer grounded in the retrieved context.

This project demonstrates practical skills in:

- Generative Artificial Intelligence
- RAG architecture
- Azure OpenAI Service
- Embeddings and vector search
- Prompt engineering
- API development with FastAPI
- PDF document processing
- Cloud service integration

This project was developed as part of the Microsoft Certification Challenge (DP-100), focusing on building real-world Machine Learning and AI solutions using Azure.

The result is a scalable, modular system ready for real-world applications such as:

- Academic assistants
- Technical support systems
- Enterprise chatbots
- Document query systems
- Scientific research assistants

## ğŸ—ï¸ System Architecture

The system implements a Retrieval-Augmented Generation (RAG) pipeline using Azure OpenAI to answer questions based on PDF documents.

Execution flow:

1. Document ingestion
   - PDFs are loaded and converted to text
   - Text is split into smaller chunks

2. Embedding generation
   - Each chunk is converted into a vector using Azure OpenAI Embeddings
   - Vectors are stored in a vector database (ChromaDB)

3. User query
   - The user question is converted into an embedding
   - The system retrieves the most relevant chunks

4. Response generation
   - Retrieved chunks are sent as context to GPT-4o
   - The model generates a response grounded in the retrieved data

Simplified architecture:

User â†’ FastAPI â†’ Embedding â†’ Vector Store â†’ Context Retrieval â†’ GPT-4o â†’ Response

## ğŸ§° Tech Stack

This project was built using modern technologies for implementing Retrieval-Augmented Generation (RAG) systems.

### â˜ï¸ Cloud & AI

- Azure OpenAI Service  
  - Generation model: GPT-4o  
  - Embedding model: text-embedding-3-large  

### ğŸ§  AI & Processing

- OpenAI Python SDK  
- ChromaDB (Vector Database)  
- Text chunking and embedding pipeline  

### âš™ï¸ Backend

- Python 3.10+
- FastAPI
- Uvicorn

### ğŸ“„ Document processing

- PyPDF
- Text ingestion and chunking pipeline

### ğŸ”§ Supporting tools

- python-dotenv (environment variable management)
- Git and GitHub (version control)
- Python virtual environment (venv)

### ğŸ§ª Development environment

- Visual Studio Code
- Windows 11

## ğŸ“ Project Structure

The project structure follows software engineering best practices, separating concerns between data ingestion, AI logic, API, and vector storage.

```
chatbot-pdf-rag-azure/
â”‚
â”œâ”€â”€ inputs/                     # Source PDFs used as knowledge base
â”‚   â”œâ”€â”€ redes_fundamentos.pdf
â”‚   â””â”€â”€ redes_topologias_protocolos.pdf
â”‚
â”œâ”€â”€ outputs/                    # Persisted vector store
â”‚   â”œâ”€â”€ vector_index.faiss
â”‚   â””â”€â”€ metadata.pkl
â”‚
â”œâ”€â”€ src/                        # Main source code
â”‚   â”œâ”€â”€ __init__.py             # Initializes Python module
â”‚   â”œâ”€â”€ api.py                  # REST API using FastAPI
â”‚   â”œâ”€â”€ chatbot.py              # Chatbot core logic (RAG)
â”‚   â”œâ”€â”€ pdf_loader.py           # PDF loading logic
â”‚   â”œâ”€â”€ text_chunker.py         # Text chunking logic
â”‚   â””â”€â”€ vector_store.py         # Vector index creation and persistence
â”‚
â”œâ”€â”€ notebooks/                  # Experiments and tests
â”‚
â”œâ”€â”€ docs/                       # Technical documentation
â”‚
â”œâ”€â”€ requirements.txt            # Project dependencies
â”‚
â”œâ”€â”€ .env                        # Environment variables (not versioned)
â”œâ”€â”€ .gitignore                  # Git ignored files
â””â”€â”€ README.md                   # Main project documentation
```
### ğŸ“Œ Component Description

- src/api.py  
  Exposes the chatbot through a REST API using FastAPI.

- src/chatbot.py  
  Implements the complete RAG pipeline:
  - Receives user questions
  - Retrieves relevant documents from vector store
  - Sends context to GPT-4o
  - Returns contextualized answer

- src/create_vector_store.py  
  Responsible for:
  - Reading PDF files
  - Chunking text
  - Generating embeddings
  - Storing vectors in ChromaDB

- chroma_db/  
  Contains the persistent vector database, allowing reuse without reprocessing documents.

- data/  
  Directory containing source documents used as knowledge base.

- .env  
  Stores sensitive credentials and Azure OpenAI configuration.

## ğŸš€ How to run locally

### 1. Clone the repository

git clone https://github.com/YOUR-USERNAME/chatbot-pdf-rag-azure.git

cd chatbot-pdf-rag-azure


### 2. Create and activate virtual environment

Windows:

python -m venv venv
venv\Scripts\activate

Linux / Mac:

python3 -m venv venv
source venv/bin/activate


### 3. Install dependencies

pip install -r requirements.txt


### 4. Configure environment variables

Create `.env` file in project root:

AZURE_OPENAI_API_KEY=your_api_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini


### 5. Add PDFs to inputs folder

inputs/
 â”œâ”€â”€ your_document.pdf


### 6. Generate vector store

python src/vector_store.py

Expected output:

outputs/vector_index.faiss
outputs/metadata.pkl


### 7. Run API

uvicorn src.api:app --reload


### 8. Access API

Open in browser:

http://127.0.0.1:8000/docs


Interactive interface available via Swagger UI.

## ğŸ“¡ API Endpoint

### Base URL

http://127.0.0.1:8000


### Query Endpoint

POST /ask


### Description

Receives a user question, performs semantic search in the FAISS vector store, and returns an answer generated by Azure OpenAI based on PDF content.


### Request Body

{
  "question": "What is a LAN network?"
}


### Response

{
  "answer": "A LAN (Local Area Network) is a computer network that covers a limited geographic area such as a home, school, or office."
}


### Example using curl

curl -X POST "http://127.0.0.1:8000/ask" \
-H "Content-Type: application/json" \
-d "{\"question\": \"What is a LAN network?\"}"


### Example using Python

import requests

url = "http://127.0.0.1:8000/ask"

data = {
    "question": "What is a LAN network?"
}

response = requests.post(url, json=data)

print(response.json())


### Interactive interface

Available at:

http://127.0.0.1:8000/docs

Allows testing the API directly from the browser.

## ğŸ’¬ Usage Examples

### Example 1 â€” Basic question

Question:

"What is a LAN network?"

Answer:

"A LAN (Local Area Network) connects devices within a limited area such as a home or office."


---

### Example 2 â€” Conceptual question

Question:

"What is the difference between LAN and WAN?"

Answer:

"LAN covers a local area, while WAN covers large geographic areas connecting multiple LANs."


---

### Example 3 â€” Technical question

Question:

"What is TCP protocol?"

Answer:

"TCP (Transmission Control Protocol) is a reliable communication protocol that ensures correct and ordered delivery of data between devices."


---

### Example 4 â€” Python usage

import requests

response = requests.post(
    "http://127.0.0.1:8000/ask",
    json={"question": "What is a router?"}
)

print(response.json())


---

### Example 5 â€” curl usage

curl -X POST "http://127.0.0.1:8000/ask" \
-H "Content-Type: application/json" \
-d "{\"question\": \"What is a network switch?\"}"


---

### Example 6 â€” Web Interface

Open in browser:

http://127.0.0.1:8000/docs

Allows testing questions directly via Swagger UI.

## ğŸš€ Possible Improvements

This project was built using a modular architecture and can be extended to production level.

### ğŸ§  AI improvements

- Support dynamic PDF upload
- Use more advanced models (ex: GPT-4o)
- Semantic re-ranking for better accuracy
- Multi-language support
- Conversation memory (persistent context)


---

### âš¡ Performance improvements

- Use dedicated vector database:
  - Azure AI Search
  - Pinecone
  - Weaviate
  - Qdrant

- Embedding cache
- Async processing
- Parallel embedding generation


---

### ğŸŒ Backend improvements

- JWT authentication
- Rate limiting
- Structured logging
- Azure Monitor integration
- Advanced error handling


---

### ğŸ–¥ï¸ Frontend improvements

- Web interface using React or Next.js
- ChatGPT-style interface
- File upload support
- Conversation history


---

### â˜ï¸ Deployment improvements

- Docker containerization
- Azure App Service deployment
- Azure Container Apps deployment
- CI/CD using GitHub Actions
- Serverless deployment


---

### ğŸ§ª Engineering improvements

- Automated tests (pytest)
- Service-oriented architecture
- Environment-based configuration
- Dev and production environment separation


---

This project follows modern AI engineering best practices and can be evolved into a full production system.